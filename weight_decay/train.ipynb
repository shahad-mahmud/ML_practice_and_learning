{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from model import Model\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data\n",
    "For this task we will use an sample salary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = 1000, 200\n",
    "n_inputs, batch_size = 200, 10\n",
    "\n",
    "weights, bias = utils.get_weights_and_bias(n_inputs)\n",
    "\n",
    "train_data = utils.create_data(weights, bias, n_train)\n",
    "test_data = utils.create_data(weights, bias, n_test)\n",
    "\n",
    "train_loader = utils.get_dataloader(train_data, batch_size=batch_size)\n",
    "test_loader = utils.get_dataloader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model \n",
    "Now we will create the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(weights, bias)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, lambda_):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            outputs = model(inputs.detach())\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(outputs, targets) + lambda_ * \\\n",
    "                torch.sum(model.weights ** 2)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}: loss: {loss.item()}')\n",
    "\n",
    "    print('L2 norm of w:', torch.norm(model.weights).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss: 0.00017794388986658305\n",
      "Epoch 20: loss: 0.00018841915880329907\n",
      "Epoch 30: loss: 0.00018843093130271882\n",
      "Epoch 40: loss: 0.00018838970572687685\n",
      "Epoch 50: loss: 0.00018840693519450724\n",
      "Epoch 60: loss: 0.00018842447025235742\n",
      "Epoch 70: loss: 0.00018840274424292147\n",
      "Epoch 80: loss: 0.00018841258133761585\n",
      "Epoch 90: loss: 0.00018838851246982813\n",
      "Epoch 100: loss: 0.00018843838188331574\n",
      "L2 norm of w: 13.886139869689941\n"
     ]
    }
   ],
   "source": [
    "# traning without weight decay\n",
    "train(epochs, lambda_=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss: 219.04275512695312\n",
      "Epoch 20: loss: 219.0427703857422\n",
      "Epoch 30: loss: 219.0427703857422\n",
      "Epoch 40: loss: 219.0427703857422\n",
      "Epoch 50: loss: 219.0427703857422\n",
      "Epoch 60: loss: 219.0427703857422\n",
      "Epoch 70: loss: 219.0427703857422\n",
      "Epoch 80: loss: 219.04278564453125\n",
      "Epoch 90: loss: 219.0427703857422\n",
      "Epoch 100: loss: 219.0427703857422\n",
      "L2 norm of w: 5.704485893249512\n"
     ]
    }
   ],
   "source": [
    "# training with weight decay\n",
    "train(epochs, lambda_=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We have implemented the weight decay from scratch with a adjustable factor lambda. Thus we can use the same script to train the model with different lambda even with out weight decay.\n",
    "\n",
    "## What we found\n",
    "While the first training, we did not use the weight decay. We notice that the training loss is very small. But during the second training, the training loss is large. Now if we look at the L2 norms of both the training, we see L2 norm of first training is about three times of the second training. This is because the weight decay is used."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c296acb6242aad5c3f7786c5a0ae309e8ef4cdc760e8462d49ca738e0119baf8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('gen_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
